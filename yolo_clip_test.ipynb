{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ff2c58-77c4-4a48-9680-df4a8619f58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP Model: openai/clip-vit-base-patch32 on device: cuda\n",
      "CLIP 모델 로드 성공.\n",
      "\n",
      "이미지 로드 성공: images/sample_hanbok_with_sneakers.jpg (550x413)\n",
      "\n",
      "--- 객체 인식 ---\n",
      "\n",
      "--- 인식 결과 (상위 5개 객체) ---\n",
      "순위 1: Korean traditional jacket, called Jeogori (확률: 0.4240)\n",
      "순위 2: A person wearing a traditional Hanbok (확률: 0.3172)\n",
      "순위 3: Korean traditional trousers, called Baji (확률: 0.2259)\n",
      "순위 4: Korean traditional hat, called Gat (확률: 0.0326)\n",
      "순위 5: Modern smartphone (확률: 0.0002)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# 1. 라이브러리 로드 및 모델 초기화\n",
    "# ----------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import gdown\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "# 사용할 모델 정의\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading CLIP Model: {CLIP_MODEL_NAME} on device: {device}\")\n",
    "\n",
    "\n",
    "# hugging face\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "try:\n",
    "    processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME, use_fast=True, use_auth_token=HF_TOKEN) \n",
    "    model = CLIPModel.from_pretrained(CLIP_MODEL_NAME, use_auth_token=HF_TOKEN).to(device)\n",
    "    print(\"CLIP 모델 로드 성공.\")\n",
    "except Exception as e:\n",
    "    if \"Unauthorized\" in str(e):\n",
    "        print(\"\\n[오류 해결 안내] 401 Unauthorized 오류가 발생했습니다. Hugging Face 토큰 인증이 필요합니다.\")\n",
    "        print(\"1. 터미널에서 'export HUGGINGFACE_TOKEN=\\\"당신의 토큰\\\"' 설정 후 다시 실행하거나,\")\n",
    "        print(\"2. 'huggingface-cli login'을 실행하여 토큰을 입력해주세요.\")\n",
    "    else:\n",
    "        # torchvision 누락 오류 등 다른 오류가 발생할 경우를 대비하여 메시지를 남김\n",
    "        print(f\"CLIP 모델 로드 실패. 라이브러리 설치 또는 기타 오류를 확인하세요: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. 한국 문화 객체 목록 정의 (검증 DB의 K_Object 테이블과 매핑됨)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# CLIP이 인식해야 할 한국 문화 객체 목록\n",
    "# 이 목록은 향후 Cloud SQL의 K_Object 테이블에서 불러와야 합니다.\n",
    "K_CULTURE_OBJECTS = [\n",
    "    \"Korean traditional hat, called Gat\", \n",
    "    \"Korean traditional jacket, called Jeogori\",\n",
    "    \"Korean traditional trousers, called Baji\",\n",
    "    \"Modern smartphone\", \n",
    "    \"A person wearing a traditional Hanbok\",\n",
    "    \"A contemporary concrete building\",\n",
    "    \"A person wearing modern sneakers\",\n",
    "]\n",
    "\n",
    "# 프롬프트 구성: CLIP의 Zero-Shot 성능을 극대화하기 위해 템플릿 사용\n",
    "# 'a photo of [객체]' 형태가 가장 일반적입니다.\n",
    "candidate_labels = [f\"a photo of {obj}\" for obj in K_CULTURE_OBJECTS]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. 이미지 준비 (GCS에서 파일 가져오기 시뮬레이션)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 테스트 이미지 로드 (예시: 로컬 파일 경로)\n",
    "# 실제 프로젝트에서는 GCS에서 파일을 직접 읽어와야 합니다.\n",
    "try:\n",
    "    # 예시 이미지 파일 경로를 설정하세요. (Graduation_Project_1 폴더 안에 저장된 테스트 이미지)\n",
    "    image_path = \"images/sample_hanbok_with_sneakers.jpg\" \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    print(f\"\\n이미지 로드 성공: {image_path} ({image.size[0]}x{image.size[1]})\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n[오류] 테스트 이미지를 찾을 수 없습니다. 프로젝트 폴더에 이미지를 저장해 주세요.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Zero-Shot Classification 실행\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n--- 객체 인식 ---\")\n",
    "\n",
    "# 프로세서로 이미지와 텍스트를 모델 입력 형태로 변환\n",
    "inputs = processor(text=candidate_labels, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# 모델 추론 실행\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "# 로짓(logit)을 확률로 변환\n",
    "logits_per_image = outputs.logits_per_image  \n",
    "probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "\n",
    "# 결과 정리 (객체 이름과 확률)\n",
    "results = []\n",
    "for i, prob in enumerate(probs):\n",
    "    results.append({\n",
    "        \"object\": K_CULTURE_OBJECTS[i],\n",
    "        \"probability\": prob\n",
    "    })\n",
    "\n",
    "# 확률이 높은 순으로 정렬\n",
    "results.sort(key=lambda x: x[\"probability\"], reverse=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. 최종 결과 출력 및 DB 매핑 준비\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n--- 인식 결과 (상위 5개 객체) ---\")\n",
    "for i, result in enumerate(results[:5]):\n",
    "    print(f\"순위 {i+1}: {result['object']} (확률: {result['probability']:.4f})\")\n",
    "    \n",
    "# **[다음 단계 준비]**\n",
    "# 1. 특정 임계값(Threshold, 예: 0.8 이상)을 넘는 객체만 최종 객체 목록으로 확정합니다.\n",
    "FINAL_RECOGNIZED_OBJECTS = [\n",
    "    result['object'] for result in results if result['probability'] > 0.8\n",
    "]\n",
    "\n",
    "#print(\"\\n--- ---\")\n",
    "print(FINAL_RECOGNIZED_OBJECTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19bb0e1a-9ee8-4c5a-9560-6a45c49aa30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8s Model for Object Detection...\n",
      "YOLOv8 모델 로드 성공.\n",
      "\n",
      "--- YOLOv8 객체 탐지 시작 (images/sample_hanbok_with_sneakers.jpg) ---\n",
      "\n",
      "[성공] 총 4개의 객체 탐지 완료.\n",
      "객체 1: person (확률: 0.86, BBox: [312, 178, 726, 1002])\n",
      "객체 2: person (확률: 0.29, BBox: [785, 539, 806, 561])\n",
      "객체 3: person (확률: 0.15, BBox: [761, 540, 779, 561])\n",
      "객체 4: person (확률: 0.12, BBox: [809, 536, 828, 562])\n",
      "\n",
      "--- 다음 단계: 각 객체 조각을 CLIP으로 정밀 분류 ---\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# 1. 라이브러리 로드 및 모델 초기화 (CLIP 부분은 잠시 주석 처리)\n",
    "# ----------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import gdown\n",
    "import cv2\n",
    "import os \n",
    "from ultralytics import YOLO # YOLOv8 라이브러리 추가\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. YOLOv8 모델 로드\n",
    "# ----------------------------------------------------\n",
    "print(\"Loading YOLOv8s Model for Object Detection...\")\n",
    "try:\n",
    "    # 'yolov8s.pt'는 작고 빠른 YOLOv8의 사전 학습된 모델입니다.\n",
    "    # GPU가 있다면 자동으로 사용됩니다.\n",
    "    yolo_model = YOLO('yolov8s.pt')\n",
    "    print(\"YOLOv8 모델 로드 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"[오류] YOLOv8 모델 로드 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. 이미지 준비 및 객체 탐지 실행\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def run_yolo_detection(image_path: str):\n",
    "    \"\"\"\n",
    "    YOLOv8을 실행하여 이미지에서 객체를 탐지하고 결과를 반환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # conf=0.25 (확률 25% 이상 객체만 탐지), iou=0.7 (중복 박스 제거 기준)\n",
    "        results = yolo_model.predict(image_path, conf=0.1, iou=0.7, save=False, verbose=False)\n",
    "        \n",
    "        detected_objects = []\n",
    "        \n",
    "        # 탐지 결과에서 Bounding Box와 클래스 이름 추출\n",
    "        for result in results:\n",
    "            if result.boxes:\n",
    "                for box in result.boxes:\n",
    "                    # BBox 좌표 (x1, y1, x2, y2)를 정수형으로 변환\n",
    "                    x1, y1, x2, y2 = [int(x) for x in box.xyxy[0].tolist()]\n",
    "                    conf = box.conf[0].item() # 확률\n",
    "                    class_id = int(box.cls[0].item())\n",
    "                    \n",
    "                    # YOLOv8의 기본 클래스 이름 가져오기 (예: 'person', 'shoe' 등)\n",
    "                    class_name = yolo_model.names[class_id] \n",
    "                    \n",
    "                    detected_objects.append({\n",
    "                        \"class_name\": class_name,\n",
    "                        \"bbox\": [x1, y1, x2, y2],\n",
    "                        \"confidence\": conf,\n",
    "                        # 다음 단계: 객체 분리 후 CLIP 분류를 위한 자리\n",
    "                        \"clip_verified_name\": None \n",
    "                    })\n",
    "\n",
    "        return detected_objects\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] YOLO 탐지 실행 중 문제 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. 테스트 실행\n",
    "# ----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"images/sample_hanbok_with_sneakers.jpg\"\n",
    "    \n",
    "    print(f\"\\n--- YOLOv8 객체 탐지 시작 ({image_path}) ---\")\n",
    "    detected_objects = run_yolo_detection(image_path)\n",
    "    \n",
    "    if not detected_objects:\n",
    "        print(\"[실패] 탐지된 객체가 없습니다. 이미지 경로와 파일 상태를 확인하세요.\")\n",
    "        exit()\n",
    "        \n",
    "    print(f\"\\n[성공] 총 {len(detected_objects)}개의 객체 탐지 완료.\")\n",
    "    \n",
    "    # 결과 출력\n",
    "    for i, obj in enumerate(detected_objects):\n",
    "        print(f\"객체 {i+1}: {obj['class_name']} (확률: {obj['confidence']:.2f}, BBox: {obj['bbox']})\")\n",
    "        \n",
    "    print(\"\\n--- 다음 단계: 각 객체 조각을 CLIP으로 정밀 분류 ---\")\n",
    "    # 여기서 각 객체의 BBox 정보를 사용하여 이미지를 잘라낸 후,\n",
    "    # 잘라낸 조각을 CLIP에 넣어 'Sneakers'인지 'Jipsin(짚신)'인지 분류해야 합니다.\n",
    "\n",
    "#``eof\n",
    "\n",
    "## 2. 다음 단계: Two-Stage Verification\n",
    "\n",
    "#위 코드를 실행하여 **YOLOv8이 이미지에서 'person', 'shoe' 같은 일반적인 객체를 잘 찾아내는지** 확인해 주세요.\n",
    "\n",
    "#1.  **YOLO 탐지:** YOLOv8이 이미지에서 **'shoe' (신발)** 객체를 찾아내고 그 위치(BBox)를 반환할 것입니다.\n",
    "#2.  **객체 조각 분리:** 반환된 BBox 좌표를 사용하여 원본 이미지에서 **신발 부분만 잘라냅니다.** \n",
    "#3.  **CLIP 정밀 분류:** 잘라낸 신발 이미지를 CLIP 모델에 넣고, 더 상세한 한국 문화 객체 목록(예: **`Modern Sneakers`**, **`Korean Jipsin`**)과 비교하여 최종적으로 **'현대 스니커즈'**임을 확정합니다.\n",
    "\n",
    "#이 과정을 거쳐야 비로소 **`Modern Sneakers`** 객체가 최종 검증 모듈로 전달되고, 고증 오류 점수 100점을 받을 수 있게 됩니다.\n",
    "\n",
    "#YOLOv8 탐지 결과가 성공적으로 나오는지 확인하신 후, **CLIP 정밀 분류를 통합하는 다음 코드**를 구현해 드리겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133909f7-7944-454a-af5c-4dbf6fa73443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP and YOLO Models on device: cuda\n",
      "CLIP 모델 로드 성공.\n",
      "YOLOv8 모델 로드 성공.\n",
      "\n",
      "[1단계 결과] 총 2개의 'person' 객체 탐지.\n",
      "  -> [오류 감지 SUCCESS] a photo of modern, white running sneakers 확정 (확률: 0.9788)\n",
      "  -> [정밀 분류] 신발 영역 최고 확률: a photo of modern, white running sneakers (확률: 0.3087)\n",
      "\n",
      "--- 고증 검증 모듈로 전달될 최종 객체 목록 ---\n",
      "객체: Modern sneakers (Anachronism), 확률: 0.9788, BBox: [312, 796, 726, 1002]\n",
      "객체: Traditional Hanbok Robe, 확률: 0.9698, BBox: [312, 178, 726, 1002]\n",
      "\n",
      "--- 다음 단계: Cloud SQL DB 연동 및 점수 산출 ---\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# 1. 라이브러리 로드 및 모델 초기화\n",
    "# ----------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from ultralytics import YOLO\n",
    "import os \n",
    "from typing import List, Dict, Any # 타입 힌트용 라이브러리 추가\n",
    "\n",
    "# 모델 설정\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "print(f\"Loading CLIP and YOLO Models on device: {device}\")\n",
    "\n",
    "# CLIP 모델 로드\n",
    "try:\n",
    "    processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME, use_fast=True, use_auth_token=HF_TOKEN) \n",
    "    model = CLIPModel.from_pretrained(CLIP_MODEL_NAME, use_auth_token=HF_TOKEN).to(device)\n",
    "    print(\"CLIP 모델 로드 성공.\")\n",
    "except Exception as e:\n",
    "    if \"Unauthorized\" in str(e):\n",
    "        print(\"\\n[오류] 401 Unauthorized 오류: Hugging Face 토큰 인증 필요.\")\n",
    "    else:\n",
    "        print(f\"[CLIP 오류] 모델 로드 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "try:\n",
    "    yolo_model = YOLO('yolov8s.pt')\n",
    "    print(\"YOLOv8 모델 로드 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"[YOLO 오류] 모델 로드 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. 정밀 분류 프롬프트 정의 (신발/의복 특화)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# CLIP이 정밀 분류해야 할 하위 객체 목록\n",
    "# 이 목록은 고증 오류를 판단하는 데 필수적인 항목들입니다.\n",
    "K_CULTURE_FINE_GRAINED_OBJECTS = [\n",
    "    # 신발 관련 프롬프트 (오류 탐지 핵심)\n",
    "    \"a photo of Korean traditional straw shoes (Jipsin)\",\n",
    "    \"a photo of Korean traditional ceremonial shoes\",\n",
    "    \"a photo of modern, white running sneakers\", \n",
    "    \"a photo of a modern casual shoe\",\n",
    "    \"a photo of the ground or a rock\", # 배경일 가능성 대비\n",
    "\n",
    "    # 의복 관련 프롬프트 (전통 의복 확정)\n",
    "    \"a photo of a high-end silk Hanbok robe\", \n",
    "    \"a photo of a common modern garment\",\n",
    "]\n",
    "\n",
    "# 프롬프트 구성\n",
    "fine_grained_labels = [f\"This object is {obj}\" for obj in K_CULTURE_FINE_GRAINED_OBJECTS]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. 객체 탐지 (YOLO) 및 정밀 분류 (CLIP) 통합 함수\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def two_stage_verify(image_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1단계: YOLO로 person 객체 탐지\n",
    "    2단계: 탐지된 person BBox를 잘라내어 CLIP으로 신발 영역 정밀 분석\n",
    "    \"\"\"\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    final_recognized_objects = []\n",
    "    \n",
    "    # --- 1단계: YOLO 객체 탐지 및 BBox 추출 ---\n",
    "    # conf=0.25 임계값을 유지하여 'person' 객체를 찾습니다.\n",
    "    yolo_results = yolo_model.predict(image_path, conf=0.25, iou=0.7, save=False, verbose=False)\n",
    "    person_bboxes = []\n",
    "\n",
    "    for result in yolo_results:\n",
    "        if result.boxes:\n",
    "            for box in result.boxes:\n",
    "                class_name = yolo_model.names[int(box.cls[0].item())]\n",
    "                if class_name == 'person':\n",
    "                    person_bboxes.append([int(x) for x in box.xyxy[0].tolist()])\n",
    "\n",
    "    print(f\"\\n[1단계 결과] 총 {len(person_bboxes)}개의 'person' 객체 탐지.\")\n",
    "\n",
    "    # --- 2단계: CLIP 정밀 분류 및 오류 객체 추출 ---\n",
    "    for bbox in person_bboxes:\n",
    "        x1, y1, x2, y2 = bbox # 사람 전체 BBox\n",
    "        \n",
    "        # [신발 영역(하단 1/4) 강제 추출 로직]\n",
    "        # 사람 전체 영역의 하단 75% 지점부터 신발 영역으로 간주하고 잘라냅니다.\n",
    "        crop_y1 = y1 + int( (y2 - y1) * 0.75 ) \n",
    "        shoe_bbox = [x1, crop_y1, x2, y2]\n",
    "        shoe_crop = original_image.crop(shoe_bbox)\n",
    "        \n",
    "        # --- CLIP 정밀 분류 실행 (신발 조각) ---\n",
    "        inputs_shoe = processor(text=fine_grained_labels, images=shoe_crop, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_shoe = model(**inputs_shoe)\n",
    "            probs_shoe = outputs_shoe.logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "             \n",
    "        # 결과 해석 및 최고 확률 객체 추출\n",
    "        shoe_results = [{\"object\": K_CULTURE_FINE_GRAINED_OBJECTS[i], \"probability\": probs_shoe[i]} for i in range(len(probs_shoe))]\n",
    "        shoe_results.sort(key=lambda x: x[\"probability\"], reverse=True)\n",
    "        top_shoe = shoe_results[0]\n",
    "        \n",
    "        # --- 최종 목록에 고증 오류 객체 및 전통 복식 객체 추가 ---\n",
    "        \n",
    "        # 1. 고증 오류 객체 확정 (스니커즈 탐지 및 임계값 0.6 설정)\n",
    "        if \"running sneakers\" in top_shoe['object'] and top_shoe['probability'] > 0.6:\n",
    "            final_recognized_objects.append({\n",
    "                \"name\": \"Modern sneakers (Anachronism)\",\n",
    "                \"probability\": top_shoe['probability'],\n",
    "                \"bbox\": shoe_bbox\n",
    "            })\n",
    "            print(f\"  -> [오류 감지 SUCCESS] {top_shoe['object']} 확정 (확률: {top_shoe['probability']:.4f})\")\n",
    "        else:\n",
    "             print(f\"  -> [정밀 분류] 신발 영역 최고 확률: {top_shoe['object']} (확률: {top_shoe['probability']:.4f})\")\n",
    "\n",
    "\n",
    "        # 2. 전통 복식 객체 추가 (고증 검증 쌍을 만들기 위해 필수)\n",
    "        # 이 코드가 없으면 DB 검증 자체가 불가능합니다.\n",
    "        # 사람 전체 BBox를 입력하여 '한복 로브'를 확정하는 로직을 재사용합니다.\n",
    "        \n",
    "        # 복식 정밀 분류 실행 (사람 전체 BBox 사용)\n",
    "        inputs_robe = processor(text=fine_grained_labels, images=original_image.crop(bbox), return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs_robe = model(**inputs_robe)\n",
    "            probs_robe = outputs_robe.logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        robe_results = [{\"object\": K_CULTURE_FINE_GRAINED_OBJECTS[i], \"probability\": probs_robe[i]} for i in range(len(probs_robe))]\n",
    "        robe_results.sort(key=lambda x: x[\"probability\"], reverse=True)\n",
    "        top_robe = robe_results[0]\n",
    "        \n",
    "        if \"Hanbok robe\" in top_robe['object'] and top_robe['probability'] > 0.8:\n",
    "             final_recognized_objects.append({\n",
    "                \"name\": \"Traditional Hanbok Robe\",\n",
    "                \"probability\": top_robe['probability'],\n",
    "                \"bbox\": bbox \n",
    "            })\n",
    "\n",
    "\n",
    "    return final_recognized_objects\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. 테스트 실행\n",
    "# ----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 이 파일은 images/sample_hanbok_with_sneakers.jpg 이미지가 있다는 가정하에 실행됩니다.\n",
    "    image_path = \"images/sample_hanbok_with_sneakers.jpg\" \n",
    "    \n",
    "    try:\n",
    "        final_objects = two_stage_verify(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n[오류] 테스트 이미지를 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "        exit()\n",
    "        \n",
    "    print(\"\\n--- 고증 검증 모듈로 전달될 최종 객체 목록 ---\")\n",
    "    if not final_objects:\n",
    "        print(\"[결과] 고증 검증 객체가 탐지되지 않았습니다.\")\n",
    "        exit()\n",
    "        \n",
    "    # 최종 결과 출력\n",
    "    for obj in final_objects:\n",
    "        print(f\"객체: {obj['name']}, 확률: {obj['probability']:.4f}, BBox: {obj.get('bbox', 'N/A')}\")\n",
    "        \n",
    "    print(\"\\n--- 다음 단계: Cloud SQL DB 연동 및 점수 산출 ---\")\n",
    "#```eof\n",
    "\n",
    "#이제 이 코드를 실행하면, **YOLO가 찾은 사람 영역의 하단(신발)에 대해 CLIP이 정밀 분류를 수행**하여, **`Modern sneakers (Anachronism)`** 객체를 높은 확률로 최종 목록에 포함시킬 것입니다.\n",
    "\n",
    "#이 결과로 **'Traditional Hanbok Robe'**와 **'Modern sneakers'**라는 두 개의 불일치 객체가 생성되어, 다음 단계인 **Cloud SQL 기반의 고증 점수 산출**로 넘길 수 있는 완벽한 뼈대가 완성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34140a0d-70fe-4eeb-ad99-c1105d8cd8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15f948-0127-456c-96cf-fe7125af21d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7205f60-6f2f-4299-ab99-f4732dd55dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (임시) DB 연동 예\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 주의: 보안을 위해 이 정보는 실제 서비스에서는 환경 변수나 Secret Manager에서 불러와야 합니다.\n",
    "DB_CONFIG = {\n",
    "    'host': 'YOUR_CLOUD_SQL_IP',     # Cloud SQL 외부 IP 주소\n",
    "    'user': 'your_db_user',           # DB 사용자 이름\n",
    "    'password': 'your_db_password',   # DB 비밀번호\n",
    "    'database': 'kculture_db',        # 데이터베이스 이름\n",
    "    'port': 3306,                     # MySQL/MariaDB 기본 포트\n",
    "    'cursorclass': pymysql.cursors.DictCursor\n",
    "}\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"DB 연결 객체를 생성하고 반환합니다.\"\"\"\n",
    "    try:\n",
    "        connection = pymysql.connect(**DB_CONFIG)\n",
    "        return connection\n",
    "    except pymysql.Error as e:\n",
    "        print(f\"ERROR: Cloud SQL 접속 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_object_info_and_rules(recognized_object_names: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    인식된 객체 목록을 기반으로 DB에서 관련 규칙 및 정보를 조회합니다.\n",
    "    \"\"\"\n",
    "    connection = get_db_connection()\n",
    "    if not connection:\n",
    "        return []\n",
    "\n",
    "    # 1. 인식된 객체 이름 목록을 DB 쿼리용 튜플로 변환\n",
    "    name_placeholders = ', '.join(['%s'] * len(recognized_object_names))\n",
    "    \n",
    "    # 2. SQL 쿼리: 인식된 객체 쌍과 관련된 모든 불일치 규칙 조회\n",
    "    # 객체 이름을 기반으로 ID를 찾아 규칙 테이블과 조인합니다.\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        r.inconsistency_score, \n",
    "        o1.object_name AS object_a, \n",
    "        o2.object_name AS object_b,\n",
    "        r.description\n",
    "    FROM Consistency_Rule r\n",
    "    JOIN K_Object o1 ON r.object_id_A = o1.object_id\n",
    "    JOIN K_Object o2 ON r.object_id_B = o2.object_id\n",
    "    WHERE o1.object_name IN ({name_placeholders}) \n",
    "      AND o2.object_name IN ({name_placeholders});\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            # 쿼리 실행: name_placeholders에 인식된 객체 이름을 두 번 넣어 줍니다.\n",
    "            params = recognized_object_names + recognized_object_names\n",
    "            cursor.execute(sql_query, params)\n",
    "            result = cursor.fetchall()\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: DB 쿼리 실행 실패: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "def calculate_anachronism_score(recognized_objects: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    인식된 객체 목록을 기반으로 고증 불일치 점수를 계산합니다.\n",
    "    \"\"\"\n",
    "    all_rules = get_object_info_and_rules(recognized_objects)\n",
    "    \n",
    "    total_score = 0\n",
    "    triggered_rules = []\n",
    "\n",
    "    # 1. 모든 인식된 객체 쌍에 대해 규칙을 확인\n",
    "    for rule in all_rules:\n",
    "        # DB에서 가져온 규칙이 현재 인식된 객체 쌍에 정확히 적용되는지 확인 (불필요한 규칙 필터링)\n",
    "        \n",
    "        # 'Modern sneakers'와 'Traditional Hanbok Robe' 같은 쌍이 있는지 확인\n",
    "        if rule['object_a'] in recognized_objects and rule['object_b'] in recognized_objects:\n",
    "            total_score += rule['inconsistency_score']\n",
    "            triggered_rules.append({\n",
    "                \"score\": rule['inconsistency_score'],\n",
    "                \"objects\": [rule['object_a'], rule['object_b']],\n",
    "                \"reason\": rule['description']\n",
    "            })\n",
    "            \n",
    "    return {\n",
    "        \"total_anachronism_score\": total_score,\n",
    "        \"triggered_rules\": triggered_rules,\n",
    "        \"judgment\": \"고증 오류 심각\" if total_score >= 100 else (\"경미한 오류\" if total_score > 0 else \"고증 적합\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2eb54-2a3b-4e73-b5c1-1bb2680d31d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99569729-2419-4ef2-a757-f4634be73286",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groundingdino'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgroundingdino\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, predict, annotate\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPProcessor, CLIPModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'groundingdino'"
     ]
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, predict, annotate\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch, numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1️⃣ Grounding DINO 로드 (객체 탐지)\n",
    "# ----------------------------------------------------\n",
    "grounding_model = load_model(\"GroundingDINO_SwinT_OGC.pth\", \"config/GroundingDINO_SwinT_OGC.py\")\n",
    "\n",
    "# 탐지할 한국문화 객체 목록\n",
    "PROMPT = \"hanbok, gat, smartphone, building, person, shoes, jeogori, baji\"\n",
    "\n",
    "image = Image.open(\"images/sample_hanbok_with_sneakers.jpg\").convert(\"RGB\")\n",
    "boxes, labels, scores = predict(\n",
    "    model=grounding_model,\n",
    "    image=image,\n",
    "    caption=PROMPT,\n",
    "    box_threshold=0.25,\n",
    "    text_threshold=0.25,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2️⃣ CLIP 로드 (세밀 분류)\n",
    "# ----------------------------------------------------\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
    "\n",
    "# fine-grained 객체 목록\n",
    "CULTURE_LABELS = [\n",
    "    \"Traditional Korean hat (Gat)\",\n",
    "    \"Traditional jacket (Jeogori)\",\n",
    "    \"Traditional pants (Baji)\",\n",
    "    \"Hanbok\",\n",
    "    \"Modern sneakers\",\n",
    "    \"Traditional straw shoes (Jipsin)\",\n",
    "    \"Modern smartphone\",\n",
    "    \"Concrete building\"\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3️⃣ 박스별 crop → CLIP 분류\n",
    "# ----------------------------------------------------\n",
    "results = []\n",
    "for i, box in enumerate(boxes):\n",
    "    cropped = image.crop(box)\n",
    "    inputs = processor(text=CULTURE_LABELS, images=cropped, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        probs = outputs.logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "\n",
    "    top_idx = np.argmax(probs)\n",
    "    results.append({\n",
    "        \"label\": CULTURE_LABELS[top_idx],\n",
    "        \"confidence\": probs[top_idx],\n",
    "        \"bbox\": box,\n",
    "        \"base_label\": labels[i],\n",
    "        \"dino_score\": float(scores[i]),\n",
    "    })\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4️⃣ 결과 출력\n",
    "# ----------------------------------------------------\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"[{i+1}] {r['label']} (CLIP: {r['confidence']:.2f}, DINO: {r['dino_score']:.2f}) BBox: {r['bbox']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69fdaad-a758-4d72-b11c-4094d2b6be54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/IDEA-Research/GroundingDINO.git\n",
      "  Cloning https://github.com/IDEA-Research/GroundingDINO.git to c:\\users\\public\\documents\\estsoft\\creatortemp\\pip-req-build-ai2c347k\n",
      "  Resolved https://github.com/IDEA-Research/GroundingDINO.git to commit 856dde20aee659246248e20734ef9ba5214f5e44\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from groundingdino==0.1.0) (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from groundingdino==0.1.0) (0.22.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from groundingdino==0.1.0) (4.57.0)\n",
      "Collecting addict (from groundingdino==0.1.0)\n",
      "  Using cached addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: yapf in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from groundingdino==0.1.0) (0.40.2)\n",
      "Requirement already satisfied: timm in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from groundingdino==0.1.0) (1.0.20)\n",
      "Requirement already satisfied: numpy in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from groundingdino==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from groundingdino==0.1.0) (4.11.0.86)\n",
      "Collecting supervision>=0.22.0 (from groundingdino==0.1.0)\n",
      "  Using cached supervision-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycocotools (from groundingdino==0.1.0)\n",
      "  Using cached pycocotools-2.0.10-cp312-abi3-win_amd64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from supervision>=0.22.0->groundingdino==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from supervision>=0.22.0->groundingdino==0.1.0) (3.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from supervision>=0.22.0->groundingdino==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from supervision>=0.22.0->groundingdino==0.1.0) (0.7.1)\n",
      "Requirement already satisfied: pillow>=9.4 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from supervision>=0.22.0->groundingdino==0.1.0) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from supervision>=0.22.0->groundingdino==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from supervision>=0.22.0->groundingdino==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from tqdm>=4.62.3->supervision>=0.22.0->groundingdino==0.1.0) (0.4.6)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from timm->groundingdino==0.1.0) (0.35.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from timm->groundingdino==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm->groundingdino==0.1.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm->groundingdino==0.1.0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm->groundingdino==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->groundingdino==0.1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->groundingdino==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->groundingdino==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->groundingdino==0.1.0) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->groundingdino==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from jinja2->torch->groundingdino==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers->groundingdino==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers->groundingdino==0.1.0) (0.22.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.6.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from yapf->groundingdino==0.1.0) (8.5.0)\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from yapf->groundingdino==0.1.0) (4.3.7)\n",
      "Requirement already satisfied: tomli>=2.0.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from yapf->groundingdino==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from importlib-metadata>=6.6.0->yapf->groundingdino==0.1.0) (3.21.0)\n",
      "Using cached supervision-0.26.1-py3-none-any.whl (207 kB)\n",
      "Using cached addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Using cached pycocotools-2.0.10-cp312-abi3-win_amd64.whl (76 kB)\n",
      "Building wheels for collected packages: groundingdino\n",
      "  Building wheel for groundingdino (setup.py): started\n",
      "  Building wheel for groundingdino (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for groundingdino\n",
      "Failed to build groundingdino\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/IDEA-Research/GroundingDINO.git 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k'\n",
      "  DEPRECATION: Building 'groundingdino' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'groundingdino'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [63 lines of output]\n",
      "  Building wheel groundingdino-0.1.0\n",
      "  Compiling with CUDA\n",
      "  running bdist_wheel\n",
      "  C:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:576: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "    warnings.warn(msg.format('we could not find ninja.'))\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-312\\groundingdino\n",
      "  copying groundingdino\\version.py -> build\\lib.win-amd64-cpython-312\\groundingdino\n",
      "  copying groundingdino\\__init__.py -> build\\lib.win-amd64-cpython-312\\groundingdino\n",
      "  creating build\\lib.win-amd64-cpython-312\\groundingdino\\config\n",
      "  copying groundingdino\\config\\GroundingDINO_SwinB_cfg.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\config\n",
      "  copying groundingdino\\config\\GroundingDINO_SwinT_OGC.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\config\n",
      "  copying groundingdino\\config\\__init__.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\config\n",
      "  creating build\\lib.win-amd64-cpython-312\\groundingdino\\datasets\n",
      "  copying groundingdino\\datasets\\cocogrounding_eval.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\datasets\n",
      "  copying groundingdino\\datasets\\transforms.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\datasets\n",
      "  copying groundingdino\\datasets\\__init__.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\datasets\n",
      "  creating build\\lib.win-amd64-cpython-312\\groundingdino\\models\n",
      "  copying groundingdino\\models\\registry.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\n",
      "  copying groundingdino\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\n",
      "  creating build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\box_ops.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\get_tokenlizer.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\inference.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\logger.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\misc.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\slconfig.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\slio.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\time_counter.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\utils.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\visualizer.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\vl_utils.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  copying groundingdino\\util\\__init__.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\util\n",
      "  creating build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\bertwarper.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\fuse_modules.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\groundingdino.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\ms_deform_attn.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\transformer.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\transformer_vanilla.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\utils.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  copying groundingdino\\models\\GroundingDINO\\__init__.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\n",
      "  creating build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\\backbone\n",
      "  copying groundingdino\\models\\GroundingDINO\\backbone\\backbone.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\\backbone\n",
      "  copying groundingdino\\models\\GroundingDINO\\backbone\\position_encoding.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\\backbone\n",
      "  copying groundingdino\\models\\GroundingDINO\\backbone\\swin_transformer.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\\backbone\n",
      "  copying groundingdino\\models\\GroundingDINO\\backbone\\__init__.py -> build\\lib.win-amd64-cpython-312\\groundingdino\\models\\GroundingDINO\\backbone\n",
      "  running build_ext\n",
      "  C:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:446: UserWarning: Error checking compiler version for cl: [WinError 2] 지정된 파일을 찾을 수 없습니다\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  C:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:480: UserWarning: The detected CUDA version (11.3) has a minor version mismatch with the version that was used to compile PyTorch (11.8). Most likely this shouldn't be a problem.\n",
      "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "  building 'groundingdino._C' extension\n",
      "  creating build\\temp.win-amd64-cpython-312\\Release\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k\\groundingdino\\models\\GroundingDINO\\csrc\\MsDeformAttn\n",
      "  \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DWITH_CUDA -IC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k\\groundingdino\\models\\GroundingDINO\\csrc -IC:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\include -IC:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include\" -IC:\\Users\\jech0\\anaconda3\\include -IC:\\Users\\jech0\\anaconda3\\Include \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\ATLMFC\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /TpC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k\\groundingdino\\models\\GroundingDINO\\csrc\\MsDeformAttn\\ms_deform_attn_cpu.cpp /Fobuild\\temp.win-amd64-cpython-312\\Release\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k\\groundingdino\\models\\GroundingDINO\\csrc\\MsDeformAttn\\ms_deform_attn_cpu.obj /MD /wd4819 /wd4251 /wd4244 /wd4267 /wd4275 /wd4018 /wd4190 /wd4624 /wd4067 /wd4068 /EHsc -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 /std:c++17\n",
      "  ms_deform_attn_cpu.cpp\n",
      "  C:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n",
      "  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "    warnings.warn(\n",
      "  \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin\\nvcc\" -c C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k\\groundingdino\\models\\GroundingDINO\\csrc\\MsDeformAttn\\ms_deform_attn_cuda.cu -o build\\temp.win-amd64-cpython-312\\Release\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k\\groundingdino\\models\\GroundingDINO\\csrc\\MsDeformAttn\\ms_deform_attn_cuda.obj -IC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-ai2c347k\\groundingdino\\models\\GroundingDINO\\csrc -IC:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\include -IC:\\Users\\jech0\\anaconda3\\Lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include\" -IC:\\Users\\jech0\\anaconda3\\include -IC:\\Users\\jech0\\anaconda3\\Include \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\ATLMFC\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcompiler /EHsc -Xcompiler /wd4068 -Xcompiler /wd4067 -Xcompiler /wd4624 -Xcompiler /wd4190 -Xcompiler /wd4018 -Xcompiler /wd4275 -Xcompiler /wd4267 -Xcompiler /wd4244 -Xcompiler /wd4251 -Xcompiler /wd4819 -Xcompiler /MD -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 -std=c++17 --use-local-env\n",
      "  nvcc fatal   : Unsupported gpu architecture 'compute_89'\n",
      "  error: command 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.3\\\\bin\\\\nvcc.EXE' failed with exit code 1\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for groundingdino\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (groundingdino)\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/IDEA-Research/GroundingDINO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9084ba40-066f-450c-96db-6d5a31c1c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\jech0\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pillow in c:\\users\\jech0\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jech0\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\jech0\\anaconda3\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python pillow matplotlib transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23df0589-6898-4107-b45b-2f2226c45d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Using cached timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from timm) (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from timm) (0.22.1+cu118)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from timm) (0.35.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torch->timm) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jech0\\anaconda3\\lib\\site-packages (from torchvision->timm) (11.0.0)\n",
      "Using cached timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-1.0.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fc607-e821-4882-a0fc-533b87bb9920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
